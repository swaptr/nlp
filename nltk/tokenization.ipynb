{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac287306-76d7-46ba-93e5-f672aa89774d",
   "metadata": {},
   "source": [
    "# tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271bfa39-7d21-4174-8b1d-790d4afe29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfc1904-f837-42e5-9594-96188165712a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/swapnil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a907454e-0ff1-4251-9b62-e699ea2c94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529c0125-1461-4a78-9d8d-6b9fda2f2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenizations is basically generating unigrams\n",
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27681ed8-e4a1-49dd-a02b-69e5f860801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'is'), ('is', 'a'), ('a', 'leading'), ('leading', 'platform'), ('platform', 'for'), ('for', 'building'), ('building', 'Python'), ('Python', 'programs'), ('programs', 'to'), ('to', 'work'), ('work', 'with'), ('with', 'human'), ('human', 'language'), ('language', 'data'), ('data', '.'), ('.', 'It'), ('It', 'provides'), ('provides', 'easy-to-use'), ('easy-to-use', 'interfaces'), ('interfaces', 'to'), ('to', 'over'), ('over', '50'), ('50', 'corpora'), ('corpora', 'and'), ('and', 'lexical'), ('lexical', 'resources'), ('resources', 'such'), ('such', 'as'), ('as', 'WordNet'), ('WordNet', ','), (',', 'along'), ('along', 'with'), ('with', 'a'), ('a', 'suite'), ('suite', 'of'), ('of', 'text'), ('text', 'processing'), ('processing', 'libraries'), ('libraries', 'for'), ('for', 'classification'), ('classification', ','), (',', 'tokenization'), ('tokenization', ','), (',', 'stemming'), ('stemming', ','), (',', 'tagging'), ('tagging', ','), (',', 'parsing'), ('parsing', ','), (',', 'and'), ('and', 'semantic'), ('semantic', 'reasoning'), ('reasoning', ','), (',', 'wrappers'), ('wrappers', 'for'), ('for', 'industrial-strength'), ('industrial-strength', 'NLP'), ('NLP', 'libraries'), ('libraries', ','), (',', 'and'), ('and', 'an'), ('an', 'active'), ('active', 'discussion'), ('discussion', 'forum'), ('forum', '.')]\n"
     ]
    }
   ],
   "source": [
    "# to generate bigrams\n",
    "print(list(nltk.bigrams(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20ceea5-b12e-4d65-b63e-9809e4b768dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'is', 'a'), ('is', 'a', 'leading'), ('a', 'leading', 'platform'), ('leading', 'platform', 'for'), ('platform', 'for', 'building'), ('for', 'building', 'Python'), ('building', 'Python', 'programs'), ('Python', 'programs', 'to'), ('programs', 'to', 'work'), ('to', 'work', 'with'), ('work', 'with', 'human'), ('with', 'human', 'language'), ('human', 'language', 'data'), ('language', 'data', '.'), ('data', '.', 'It'), ('.', 'It', 'provides'), ('It', 'provides', 'easy-to-use'), ('provides', 'easy-to-use', 'interfaces'), ('easy-to-use', 'interfaces', 'to'), ('interfaces', 'to', 'over'), ('to', 'over', '50'), ('over', '50', 'corpora'), ('50', 'corpora', 'and'), ('corpora', 'and', 'lexical'), ('and', 'lexical', 'resources'), ('lexical', 'resources', 'such'), ('resources', 'such', 'as'), ('such', 'as', 'WordNet'), ('as', 'WordNet', ','), ('WordNet', ',', 'along'), (',', 'along', 'with'), ('along', 'with', 'a'), ('with', 'a', 'suite'), ('a', 'suite', 'of'), ('suite', 'of', 'text'), ('of', 'text', 'processing'), ('text', 'processing', 'libraries'), ('processing', 'libraries', 'for'), ('libraries', 'for', 'classification'), ('for', 'classification', ','), ('classification', ',', 'tokenization'), (',', 'tokenization', ','), ('tokenization', ',', 'stemming'), (',', 'stemming', ','), ('stemming', ',', 'tagging'), (',', 'tagging', ','), ('tagging', ',', 'parsing'), (',', 'parsing', ','), ('parsing', ',', 'and'), (',', 'and', 'semantic'), ('and', 'semantic', 'reasoning'), ('semantic', 'reasoning', ','), ('reasoning', ',', 'wrappers'), (',', 'wrappers', 'for'), ('wrappers', 'for', 'industrial-strength'), ('for', 'industrial-strength', 'NLP'), ('industrial-strength', 'NLP', 'libraries'), ('NLP', 'libraries', ','), ('libraries', ',', 'and'), (',', 'and', 'an'), ('and', 'an', 'active'), ('an', 'active', 'discussion'), ('active', 'discussion', 'forum'), ('discussion', 'forum', '.')]\n"
     ]
    }
   ],
   "source": [
    "# to generate trigrams\n",
    "print(list(nltk.trigrams(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ddae3b8-baf5-4b7c-bc72-6401f048c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('leading', 'VBG')]\n",
      "[('platform', 'NN')]\n",
      "[('for', 'IN')]\n",
      "[('building', 'NN')]\n",
      "[('Python', 'NN')]\n",
      "[('programs', 'NNS')]\n",
      "[('to', 'TO')]\n",
      "[('work', 'NN')]\n",
      "[('with', 'IN')]\n",
      "[('human', 'NN')]\n",
      "[('language', 'NN')]\n",
      "[('data', 'NNS')]\n",
      "[('.', '.')]\n",
      "[('It', 'PRP')]\n",
      "[('provides', 'VBZ')]\n",
      "[('easy-to-use', 'NN')]\n",
      "[('interfaces', 'NNS')]\n",
      "[('to', 'TO')]\n",
      "[('over', 'IN')]\n",
      "[('50', 'CD')]\n",
      "[('corpora', 'NNS')]\n",
      "[('and', 'CC')]\n",
      "[('lexical', 'JJ')]\n",
      "[('resources', 'NNS')]\n",
      "[('such', 'JJ')]\n",
      "[('as', 'IN')]\n",
      "[('WordNet', 'NN')]\n",
      "[(',', ',')]\n",
      "[('along', 'IN')]\n",
      "[('with', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('suite', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('text', 'NN')]\n",
      "[('processing', 'NN')]\n",
      "[('libraries', 'NNS')]\n",
      "[('for', 'IN')]\n",
      "[('classification', 'NN')]\n",
      "[(',', ',')]\n",
      "[('tokenization', 'NN')]\n",
      "[(',', ',')]\n",
      "[('stemming', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('tagging', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('parsing', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('and', 'CC')]\n",
      "[('semantic', 'JJ')]\n",
      "[('reasoning', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('wrappers', 'NNS')]\n",
      "[('for', 'IN')]\n",
      "[('industrial-strength', 'NN')]\n",
      "[('NLP', 'NN')]\n",
      "[('libraries', 'NNS')]\n",
      "[(',', ',')]\n",
      "[('and', 'CC')]\n",
      "[('an', 'DT')]\n",
      "[('active', 'JJ')]\n",
      "[('discussion', 'NN')]\n",
      "[('forum', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/swapnil/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# part of speech tagging\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "for token in tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c40421d-7f4d-4c9d-91d3-e2691bf13d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('leading', 'VBG')]\n",
      "[('platform', 'NN')]\n",
      "[('for', 'IN')]\n",
      "[('building', 'NN')]\n",
      "[('Python', 'NN')]\n",
      "[('programs', 'NNS')]\n",
      "[('to', 'TO')]\n",
      "[('work', 'NN')]\n",
      "[('with', 'IN')]\n",
      "[('human', 'NN')]\n",
      "[('language', 'NN')]\n",
      "[('data', 'NNS')]\n",
      "[('.', '.')]\n",
      "[('It', 'PRP')]\n",
      "[('provides', 'VBZ')]\n",
      "[('easy', 'JJ')]\n",
      "[('-to-use', 'NN')]\n",
      "[('interfaces', 'NNS')]\n",
      "[('to', 'TO')]\n",
      "[('over', 'IN')]\n",
      "[('50', 'CD')]\n",
      "[('corpora', 'NNS')]\n",
      "[('and', 'CC')]\n",
      "[('lexical', 'JJ')]\n",
      "[('resources', 'NNS')]\n",
      "[('such', 'JJ')]\n",
      "[('as', 'IN')]\n",
      "[('WordNet', 'NN')]\n",
      "[(',', ',')]\n",
      "[('along', 'IN')]\n",
      "[('with', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('suite', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('text', 'NN')]\n",
      "[('processing', 'NN')]\n",
      "[('libraries', 'NNS')]\n",
      "[('for', 'IN')]\n",
      "[('classification', 'NN')]\n",
      "[(',', ',')]\n",
      "[('tokenization', 'NN')]\n",
      "[(',', ',')]\n",
      "[('stemming', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('tagging', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('parsing', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('and', 'CC')]\n",
      "[('semantic', 'JJ')]\n",
      "[('reasoning', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('wrappers', 'NNS')]\n",
      "[('for', 'IN')]\n",
      "[('industrial', 'JJ')]\n",
      "[('-strength', 'NN')]\n",
      "[('NLP', 'NN')]\n",
      "[('libraries', 'NNS')]\n",
      "[(',', ',')]\n",
      "[('and', 'CC')]\n",
      "[('an', 'DT')]\n",
      "[('active', 'JJ')]\n",
      "[('discussion', 'NN')]\n",
      "[('forum', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# regexp tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_exp_tokenizer = RegexpTokenizer(r'\\w+|$[0-9]+|\\S+')\n",
    "tokens = reg_exp_tokenizer.tokenize(text)\n",
    "for token in tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b99ccb-5d58-4761-a837-628db4bee449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.', 'It', 'provides', 'easy', '-to-use', 'interfaces', '50', 'corpora', 'lexical', 'resources', 'WordNet', ',', 'along', 'suite', 'text', 'processing', 'libraries', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'semantic', 'reasoning', ',', 'wrappers', 'industrial', '-strength', 'NLP', 'libraries', ',', 'active', 'discussion', 'forum', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/swapnil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stop words removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "filtered_data = [word for word in tokens if not word in stop_words]\n",
    "print(filtered_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
